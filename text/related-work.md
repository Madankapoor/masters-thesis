# Related Work

Over the years there have been many attempts to automate bug triage. First effort was made by Čubranić and Murphy [cite1] who used a text categorization approach with a Naive Bayes classifier on Eclipse data. Their feature vector included a bag of words constructed from the data with stop words and punctuation removed. They achieved about 30% accuracy and among others showed that stemming has no real effect on classification performance, which is a main reason why many future efforts did not use it.

Anvik et al. [cite2] also chose a text categorization approach but instead of a Naive Bayes classifier, they used Support Vector Machine (SVM) on Eclipse, Firefox and GCC data. Similarly to [cite1], they used a bag of words with stop words removed. For Firefox data, if a report is resolved as FIXED, they labeled the data with the name of the developer who submitted the last patch, for Eclipse data, the name of the developer who marked the report as resolved was used. DUPLICATE reports were labeled by the names of those who resolved the original report, both for Eclipse and Firefox. WORKSFORME (only applicable to Firefox) reports were removed from the dataset. In total only 1% of Eclipse data was deemed unclassifiable and thus removed, in contrast 49% of Firefox data was removed. With this approach, precision of 64% and 58% was achieved on Firefox and Eclipse data respectively, however, only 6% on GCC data. As for recall, only 2%, 7% and 0.3% results were achieved on Firefox, Eclipse and GCC data respectively.

Ahsan et al. [cite3] used an SVM classifier on Mozilla data. Text terms were weighted using simple term frequency (TF) and term frequency inverse document frequency (TF-IDF). Furthermore, they reduced the number of features by applying feature selection (all terms that did not appear in at least 3 documents were removed from the feature vector) and latent semantic indexing (LSI). Only resolved and fixed bug reports were considered, the rest such as duplicate were removed from the training dataset. The data was labeled by the name of the developer who changed the status of the bug report to resolved. This approach has 44.4% classification accuracy, 30% precision and 28% recall using TF with LSI.

All previous efforts used supervised learning approaches, Xuan et al. [cite4] used a semi-supervised text classification to avoid the deficiency of unlabelled bug reports in supervised approaches. Using Eclipse data, they train their classifier in two steps. First, a basic classifier is build on labeled data, second, the classifier is improved by labeling the unlabelled dataset with current classifier and rebuilding the classifier on all data. Naive Bayes classifier with a weighted recommendation list (used to further improve the second step of training the classifier) is employed. With this setting, the classifier achieves a maximum of 48% accuracy.

Shokripour et al. [cite5] chose an entirely different approach that I will mention only briefly as versioning data were needed for bug assignment. Instead of text classification used previously, Information Extraction methods on versioning repositories of Eclipse, Mozilla and Gnome projects were employed accomplishing recall values of 62%, 43% and 41% respectively.

Quite an extensive study was done by Mamdouh et al. [cite6], who used a Naive Bayes classifier on Eclipse-SWT, Eclipse-UI, NetBeans and Maemo. What makes their research different from others is their feature selection. 5 different methods were used to reduce dimensionality of the feature vector, namely Log Odds Ratio (LOG), which measures the odds of a term occurring in a positive class normalized by the negative class, χ^2, which examines the independents of a term in a class, Term Frequency Relevance Frequency (TFRF), Mutual Information (MI) and Distinguishing Feature Selector (DFS). Best results were achieved using χ^2 resulting in precision values of 38%, 50%, 50% and 50% and recall values of 30%, 35%, 21% and 46% on Eclipse-SWI, Eclipse-UI, NetBeans and Maemo projects respectively.

Somasundaram and Murphy [cite7] used SVM model with Latent Dirichlet Allocation (LDA) feature selection and Kullback Leibler divergence (KL) with LDA. They tested their models on Eclipse, Mylyn and Bugzilla data and claim to be using recall metric for comparison, although it must be pointed out that the equation they used for the computation of recall matches equation for accuracy, not recall, this is probably an error. For Bugzilla data, the number of components (categories) is 26 and the amount of training data is 6832. With this setting, they achieved recall values of 77 % and 82 % on SVM-LDA and LDA-KL models respectively. In addition, the LDA-KL model seems to produce more consistent results when the number of categories changes.
