# Related Work

Over the years there have been many attempts to automate bug triage. First effort was made by Čubranić and Murphy [cite1] who used a text categorization approach with a Naive Bayes classifier on Eclipse data. Their feature vector included a bag of words constructed from the data with stop words and punctuation removed paving the way for subsequent attempts of automatic bug assignment. Number of reports in their dataset is 15,670 with 162 classes while the downloaded reports were created in a span of half a year. They achieved about 30% accuracy and also showed that stemming has no real effect on classification performance, which is a main reason why many future efforts do not use it. Another interesting conclusion is that removing words from feature vector occurring less than *x*-times seems to decrease the performance rather than increase it.

Anvik et al. [cite2] also chose a text categorization approach but instead of a Naive Bayes classifier, they used Support Vector Machine (SVM) on Eclipse, Firefox and GCC data. Similarly to [cite1], they used a bag of words with stop words removed. The amount of reports in the Eclipse dataset was 8655 and 9752 in the Firefox dataset. Reports from both datasets were created in a span of half a year. For Firefox data, if a report was resolved as FIXED, they labeled the data with the name of the developer who submitted the last patch, for Eclipse data, the name of the developer who marked the report as resolved was used. DUPLICATE reports were labeled by the names of those who resolved the original report, both for Eclipse and Firefox. WORKSFORME (only applicable to Firefox) reports were removed from the dataset. In total only 1% of Eclipse data was deemed unclassifiable and thus removed, in contrast 49% of Firefox data was removed. With this approach, precision of 64% and 58% was achieved on Firefox and Eclipse data respectively, however, only 6% on GCC data. As for recall, only 2%, 7% and 0.3% results were achieved on Firefox, Eclipse and GCC data respectively.

Ahsan et al. [cite3] used an SVM classifier on Mozilla data. Text terms were weighted using simple term frequency (TF) and term frequency inverse document frequency (TF-IDF). Furthermore, they reduced the number of features by applying feature selection (all terms that did not appear in at least 3 documents were removed from the feature vector) and latent semantic indexing (LSI). Only resolved and fixed bug reports were considered, the rest such as duplicate were removed from the training dataset. In total, 792 bug reports were used for this classifier with 18 labels after removing reports that were fixed by developers that had not fixed at least 30 bugs in this dataset. The data was labeled by the name of the developer who had changed the status of the bug report to resolved. This approach has 44.4% classification accuracy, 30% precision and 28% recall using SVM with LSI.

All previous efforts used supervised learning approaches, Xuan et al. [cite4] used a semi-supervised text classification to avoid the deficiency of unlabelled bug reports in supervised approaches. Using Eclipse data, they trained their classifier in two steps. First, a basic classifier was built on labeled data, second, the classifier was improved by labeling the unlabelled dataset with current classifier and rebuilding the classifier on all data. The number of reports was 5050 with 60 labels after removing all reports that were fixed by developers that resolved less than 40 of them. As for the classifier, Naive Bayes with a weighted recommendation list (used to further improve the second step of training the classifier) was employed. With this setting, the classifier achieves a maximum of 48% accuracy for top-5 recommendations but only 21% for top-1 recommendation.

Shokripour et al. [cite5] chose an entirely different approach that I will mention only briefly as versioning data were needed for bug assignment. Instead of text classification used previously, Information Extraction methods on versioning repositories of Eclipse, Mozilla and Gnome projects were employed accomplishing recall values of 62%, 43% and 41% respectively for top-5 recommendations.

Quite an extensive study was done by Alenezi et al. [cite6], who used a Naive Bayes classifier on Eclipse-SWT, Eclipse-UI, NetBeans and Maemo. Bugs that were fixed by developers who resolved less than 25 bugs in the last year were removed from the dataset resulting in the size of the datasets equal to 6560, 6104, 9284 and 4659 reports for Eclipse-SWT, Eclipse-UI, NetBeans and Maemo respectively. What makes their research different from others is their feature selection. 5 different methods were used to reduce dimensionality of the feature vector, namely Log Odds Ratio (LOG), which measures the odds of a term occurring in a positive class normalized by the negative class, χ^2, which examines the independents of a term in a class, Term Frequency Relevance Frequency (TFRF), Mutual Information (MI) and Distinguishing Feature Selector (DFS). Best results were achieved using χ^2 resulting in precision values of 38%, 50%, 50% and 50% and recall values of 30%, 35%, 21% and 46% on Eclipse-SWI, Eclipse-UI, NetBeans and Maemo projects respectively.

Somasundaram and Murphy [cite7] used SVM model with Latent Dirichlet Allocation (LDA) feature selection and Kullback Leibler divergence (KL) with LDA. They tested their models on Eclipse, Mylyn and Bugzilla data and claim to be using recall metric for comparison, although it must be pointed out that the equation they used for the computation of recall matches equation for accuracy, not recall, this is probably an error either in the metric they actually used or in the paper. For Bugzilla data, the number of components (categories) is 26 and the amount of training data is 6832. With this setting, they achieved recall values of 77% and 82% on SVM-LDA and LDA-KL models respectively. In addition, the LDA-KL model seems to produce more consistent results when the number of categories changes.

Very interesting idea is to use Content-based Recommendation (CBR) or Collaborative Filtering (CF). One of the interesting attempts was conducted by Park et al. [cite8]. They employed a Content-boosted Collaborative Filtering (CBCF), which is the combination of the two mentioned recommender algorithms, with a cost-aware triage algorithm CosTriage that tries to prevent overloading developers. The data used for the training was downloaded from Apache, Eclipse, Linux kernel and Mozilla projects. From these datasets, they removed all bugs that were fixed by inactive developers (determined by interquartile range) resulting in datasets of 656 reports assigned to 10 developers for Apache, 47,862 reports and 100 developers for Eclipse, 968 reports and 28 developers for Linux kernel and 48,424 reports assigned to 117 developers for Mozilla. Reports from all repositories were created in a period of about 8 to 12 years. In this setting, accuracy equal to values 64%, 35%, 25% and 59% was achieved on Apache, Eclipse, Linux kernel and Mozilla projects respectively while taking into account cost of each developer recommendation.

Thung et al. chose a semi-supervised learning algorithm for bug classification to 3 defect families and were able to achieve some very good results. This approach combines clustering, active-learning and semi-supervised learning algorithms and the main feature is that it requires very few labeled samples to achieve good classification results. The used dataset consists of 500 bug reports from Mahout, Lucene and OpenNLP projects. This model is able to achieve a weighted precision, recall, F-measure and AUC of 0.651, 0.669, 0.623, and 0.710 respectively.