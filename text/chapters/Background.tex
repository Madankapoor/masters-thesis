\chapter{Background}
\label{chapter:background}

Our thesis could not exist without some fundamentals that were developed by researches long time before. In this chapter, we dive into these fundamentals to get a better intuition about their theory and inner workings. First, we discuss several basic principals in Machine Learning and Text Classification including supervised vs unsupervised learning, regression vs classification and binary-class vs multi-class classification. Next, we describe some ML models like Support Vector Machine and Naive Bayes. In the last section, we detail several ML Feature Extraction methods.

\section{Machine Learning and Text Classification}

Machine Learning is a field of study in \textit{Artificial Intelligence} (AI) that enables programing of intelligent computer software that is able to learn from experience to improve its performance~\cite{Samuel1959}. This definition fits a very broad amount of techniques including linear and logistic regression, clustering, decision tree learning, probabilistic classification, deep learning using artificial neural networks and many more~\cite{bishop2006pattern}. Our thesis uses these ML techniques to achieve text classification.

Text classification (or document classification) is a problem in computer science that attempts to assign documents to several categories (classes)~\cite{Anvik2006}. A simple example is spam detection where the goal is to determine whether an incoming email is an unsolicited message and should not therefore be presented to the user. Another more complex example is to classify books by their descriptions into several categories based on genres or sentiment analysis of a review (determining the attitude of the reviewer~\cite{pang2008sentiment} -- which can help us determine public opinion about some topic or customer's opinion about a product). Machine Learning allows us to solve this problem without some intricate user intervention.

In the following text, we briefly show how basic learning algorithms work. This will allow us to understand more complicated topics essential for the next two sections. We also explore some basic ML classes (or categories) relevant for this thesis that are used to distinguish ML algorithms into groups.

\subsection{Basic Mechanics}

Generally, machine learning algorithms are based on statistical and probabilistic concepts (as well as decision and information theory) that allow them to use some existing dataset to extrapolate a prediction for future data samples~\cite{bishop2006pattern}. This is very similar in nature to what mathematical statistics (statistical inference in particular) attempts, i.e. the goal is to get some general idea about a population based solely on a dataset sample with limited size~\cite{upton2014dictionary}. An example is regression analysis where one of the goals is to plot a line or a function that fits the data as well as possible~\cite{fox1997applied}. This analysis allows us to predict (or estimate) an output for a sample without already having the sample in our dataset. The remainder of this section is based on~\cite{mitchell1997machine}.

Learning algorithms are implemented in many ways. Most of them use data from previous experience (called \textit{training data}) to optimize a \textit{cost function} based on a \textit{hypothesis function}. A cost function usually takes one or more parameters (usually represented as a vector or a matrix $\theta$) that are iteratively modified in order to achieve the lowest possible value for its output value. The cost function represents some (weighted) distance from some or all of the samples from the training dataset. It can, however, encompass more complicated functionality in order to achieve better performance. A hypothesis function is a function that is used for extrapolating predictions. An example of a cost function with a hypothesis function $h_\theta$ is shown below ($m$ is number of data samples from training dataset, $x^{(i)}$ is a vector representing a data sample and $y^{(i)}$ is the output value associated with this sample that we want to predict for future samples).

$$J(\theta) = \frac{1}{2m}\sum_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)})^2$$

This cost function penalizes the hypothesis function if the function does not fit the data well. The goal is therefore to find a $\theta$ parameter that shifts the hypothesis function towards a better fit causing decrease in the cost function output value.

There are other forms of learning algorithms, this however demonstrates how a general learning algorithm can be constructed. To summarize, the goal is usually very simple -- optimize a mathematical model to fit an existing dataset as well as possible. Based on the constructed mathematical model, predict the output for subsequent data samples to aid the user in their task.

\subsection{Regression and Classification}

Now we look at some categories by which learning algorithm can be classified. The first classification is based on the output type the learning algorithm tries to predict.

Regression deals with predicting some continues output value based on a data sample~\cite{worster2007understanding}. An example is the prediction of the size of a tumor based on biopsy. In other words, we have some known measured features and we want to predict a value based on it that we do not know, where the value is continuous in nature. The most common learning algorithm that addresses regression is \textit{linear regression}.

Classification tries to determine a class that a data sample can be labeled with~\cite{worster2007understanding}. A good example is trying to predict if a tumor is malignant or benign based on biopsy. The predicted value is always discrete as there must be a finite number of classes. Most common number of classes is two (binary-class classification), but there can be many more (multi-class classification). An example of a learning algorithm that deals with classification is \textit{logistic regression}.

To summarize, the difference is in the type of output value (the dependent variable) that these two methods predict. Regression predicts a single continuous value while classification divides data sample into discrete finite categories. Our thesis uses multi-class classification to achieve its goal.

\subsection{Supervised and Unsupervised Learning}

Another way to classify learning algorithms is by the way they are trained. The most important such categories distinguish learning algorithms into supervised and supervised classes. Less common is semi-supervised learning which is a combination of both.

Supervised learning algorithms use labeled training examples to construct the inner mathematical model used for prediction~\cite{donalek2014supervised}. The problems these learning algorithm try to solve must have available output values for training dataset and these past output values must be as accurate as possible otherwise the performance of the learning algorithm will be low. An example of a problem that can be addressed by a supervised learning algorithm is the prediction of a digit drawn by a human hand. We can easily label all training example because we are able to recognize what digit is on each example. A common supervised learning algorithm is logistic regression.

Unsupervised learning algorithms, on the contrary, do not require labeled training dataset~\cite{donalek2014supervised}. Instead, these algorithms try to find some pattern in the data by which to cluster them~\cite{donalek2014supervised}. The problems solved by these algorithms do not have known output values or the amount of possible output values is not predetermined. An example of an unsupervised problem is clustering of images into some unknown categories. An example of an unsupervised learning algorithm is k-means that tries to optimize the location of $k$ \textit{centroids} so that the total distance of all other samples to nearest centroids is as short as possible~\cite{mitchell1997machine}.

The problem of our thesis can be solved with supervised learning as the assignees are \textit{usually} known. Sometimes, however, the assignee might not be known in which case the algorithm can be aided by an unsupervised learning algorithm, resulting in a semi-supervised approach. The lack of known assignees on past examples can be more substantial on some bug repositories with more irresponsible users, in which case a semi-supervised approach can be a benefit. We, however, focus only on supervised learning in our work.

\section{Classification Models}

Classification models we use in our thesis are described in this section. As we established in the previous section, our problem falls under supervised text classification. With this information only, we can determine what learning algorithms are most suitable and which models we can safely disregarded.

After studying related work (see \autoref{chapter:related-work}), we consider three ML algorithms starting off with \textit{Naive Bayes} (NB) as that is the most usual choice for this problem because of its simplicity and relatively good performance. After that, we discuss \textit{Decision Tree Learning} and finally we conclude with \textit{Support Vector Machine} (SVM) -- one of the most prominent text classification techniques discovered in the second half of the previous century~\cite{vapnik1995svm}.

\subsection{Naive Bayes}

The first supervised classification model we introduce is the Naive Bayes. It is probably the most intuitive way of doing text classification as it relies on probability to predict the most likely class for a document~\cite{manning2008introduction}. The idea is quite simple, compute the conditional probability $P$ that document $d$ belongs to class $c$. Formally (from~\cite{manning2008introduction}):

$$P(c|d) \propto P(c) \prod_{1 \leq k \leq n_d} P(t_k|c)$$ 

$P(c)$ is the probability of a document belonging to class $c$. $P(t_k|c)$ is the conditional probability that term $t_k$ occurs in a document that belongs to class $c$ and $n_d$ is number of documents. This formula is based on Bayes' theorem, it, however, ignores the denominator of the full equation as it is not needed for finding the most probable class~\cite{hand2001bayes}. Another important note is that the formula above assumes the $P(t_k|c)$ probabilities are independent (i.e. the occurrence of term $t_k$ does not decrease or increase the probability of occurrence of term $t_l$). In practice, it is unlikely that all terms are independent, if there is the term \textit{Machine} in a document, it probably slightly increases the probability that there is also the term \textit{Learning}. This naive assumption (called strong independence relation assumption) is the reason for the name of the classification model~\cite{hand2001bayes}.

The goal of a classifier based on NB is to predict which class is the most likely for a document. The first step (training) is to determine the probabilities $P(c)$ and $P(t_k|c)$~\cite{manning2008introduction}. Unfortunately, it is only possible to \textit{estimate} these probabilities based on previous labeled documents, the amount of these documents increases the accuracy of the probabilities. We will write $\hat{P}$ instead of $P$ to refer to the estimated probabilities. The second step (prediction) is to compute the most likely or \textit{maximum a posteriori} (MAP) class $c_{map}$ like this ($C$ is a set of all classes)~\cite{manning2008introduction}:

$$c_{map} = \argmax_{c \in C} \hat{P}(c|d) = \argmax_{c \in C} \hat{P}(c) \prod_{1 \leq k \leq n_d} \hat{P}(t_k|c)$$

Naive Bayes is not only very simple intuitive way of text classification, it is also relatively fast having quadratic training time complexity $\Theta(|C||V|)$ (ignoring preprocessing of documents) where $|C|$ is number of classes and $|V|$ is the size of the vocabulary~\cite{manning2008introduction}. This makes NB quite useful in many scenarios despite its lower performance in some applications.

\subsection{Decision Tree Learning}

Decision tree learning uses decision tree as a classification model~\cite{mitchell1997machine}. There are many algorithms that can construct such a decision tree based on some labeled dataset~\cite{mitchell1997machine}, including C4.5, C5.0, CART etc. In this section, we do not discuss how such a tree can be constructed but focus only on the general properties of these algorithms and on the way the tree is used for prediction.

The general concept of this algorithm is very simple, each node represents a feature (e.g. color, size) and edges represent possible values of the parent node (e.g. red, green, small, big)~\cite{mitchell1997machine}. The leaves of the decision tree are used for predictions~\cite{mitchell1997machine}. As an example, imagine you want to predict what kind of vehicle is on a picture (after some preprocessing). The first level of a decision tree that predicts this can contain feature \textit{number of wheels}. Second layer can represent a boolean feature \textit{has engine}. The leaves would represent predictions, e.g. car, motorcycle, bicycle, boat etc. You can see such decision tree on figure~\ref{fig:decision.tree.example}.

\begin{figure}[htbp]
    \centering
        \begin{tikzpicture}
          [auto,node/.style={rectangle, draw, text centered, text width=1.5cm,minimum height=1cm },node distance=2cm]
        \tikzset{
          level 1/.style={sibling distance = 4cm, level distance=3cm, edge from parent path={(\tikzparentnode.south) -> (\tikzchildnode.north)}}
        }
          \node [node] {Wheels}
            child {node {\textit{Boat}} edge from parent node[above left] {0}}
            child {node [node] {Engine?}
                child {node {\textit{Motorcycle}} edge from parent node[above left] {Yes}}
                child {node {\textit{Bicycle}} edge from parent node[above right] {No}}
                edge from parent node[above left] {2}}
            child {node {\textit{Car}} edge from parent node[above right] {4}};
        \end{tikzpicture}
    \caption{Example of a decision tree}
    \label{fig:decision.tree.example}
\end{figure}

To predict the type of vehicle, the algorithm first examines the top level feature (in our example number of wheels) and based on the value of the sample it proceeds to the lower level. If the chosen node is a leaf, it returns prediction. Otherwise it recursively proceeds the same way it did at the top level~\cite{mitchell1997machine}. Let's say we have a picture with a vehicle and we process the image determining it contains a vehicle with two wheels and an engine. Using our decision tree, we fist examine node number of wheels and proceed to middle node engine. Our process showed us the vehicle has engine thus we proceed to along edge \textit{Yes} predicting the vehicle is a motorcycle.

There are a couple of problems in our example, however. It is incomplete, there are vehicles with 3, 5 or more wheels. There are also vehicles that have 4 wheels but are not a car (e.g. truck). Another problem is that it could be hard or nearly impossible to process an image and determine from it if the vehicle has an engine or does not and therefore choosing engine as the second feature would be impractical.

Applying decision tree learning on text classification is usually done by counting word occurrences in a document and passing them through a pre-constructed decision tree. Some decision tree learning algorithms (called regression trees) can even predict outcomes that can be considered real values instead of categories~\cite{loh2011cart}. In our evaluation, we use algorithm CART that enables it.

\subsection{Support Vector Machine}

The Support Vector Machine (SVM) is a state-of-the-art machine learning algorithm introduced in 1992 by Boser, Guyon, and Vapnik~\cite{boser1992training}. It has been widely used in bioinformatics~\cite{byvatov2002support} as well as in text classification due to its ability to deal with high-dimensional data while maintaining good accuracy~\cite{joachims1998text}. SVMs belong to a class of algorithms for pattern analysis called \textit{kernel methods}. Kernel methods enable us to operate in a high-dimensional feature space without the need to compute the coordinates of the data in that space, but rather by simply computing the inner products between the images of all pairs of data in the feature space~\cite{Ben-hur}\cite{manning2008introduction}. In this section, we will first look at how SVMs with simple linear classification operate and then look at how more advanced classification can be done with kernel functions.

\begin{figure}[htbp]
    \centering
        \includegraphics[page=3,trim=5.5cm 16cm 4cm 6.5cm,clip,width=\textwidth]{./images/external/intro-ir-svm.pdf}
    \caption{Depiction of SVM decision boundary with support vectors and margins (image taken from~\cite{manning2008introduction})}
    \label{fig:background.svm.decision.intro}
\end{figure}

The general way SVMs operate is straightforward. Instead of trying to find a function that fits training data as well as possible by optimizing the distance from samples to the function, SVM algorithm attempts to find a linear separator of positive and negative samples by optimizing the margin of the decision boundary (hyperplane)~\cite{manning2008introduction}. See figure~\ref{fig:background.svm.decision.intro}. As you can see, the samples that limit the margin of the decision boundary on the left and right side are called support vectors, there is always at least two samples on at least one side in a fully optimized SVM, otherwise the margins could still be further maximized.

You can probably spot a problem in this mathematical model. The positive and negative samples can be mixed up together so much that there simply is not a room for an applicable decision boundary. This is why the strictness of the optimization algorithm can be relaxed by tuning the regularization parameter $C$~\cite{manning2008introduction}. Setting this parameter to a higher value allows the optimization algorithm to ignore some positive or negative examples (called \textit{outliers}) on the wrong side of the decision hyperplane~\cite{manning2008introduction}.

We will now proceed to have a look at non-linear classification with SVMs. In many applications, linear classification is not enough to warrant a good decision boundary and therefore performance. For this reason, we define non-linear classification that can deal with data that follow a more complicated structure. Using kernel functions, we can effectively transform the decision boundary into polynomial, Gaussian or sigmoid function. Due to using technique called \textit{kernel trick}, this can be done without increasing the time complexity of the learning algorithm~\cite{manning2008introduction}.

\section{Feature Extraction Methods}

We now proceed to examine several feature extraction methods. These methods can highly increase the performance of a classification model and there can be many techniques used in sequence. The essential feature extraction technique is determining number of occurrences of a word in a document (word frequency). This method is almost always used (before all other techniques are applied) as it is simple and yields high performance. In this section, we examine other feature extraction techniques that are applied on top of this essential method.

\subsection{Stop-Words Removal}

Stop-words removal is a feature extraction method that removes unnecessary words. These words are usually downloaded from a corpus as a stop-word list. A stop-word is a word that contain little meaning (a, the, and, at etc.)~\cite{scott1999feature}. This technique is very simple and increases the performance significantly, it is therefore almost always used in conjunction with other feature extraction methods~\cite{scott1999feature}. 

\subsection{Term Frequency--Inverse Document Frequency}

Term Frequency--Inverse Document Frequency (TF--IDF) is a numerical statistic that used to determine the importance of a word in a document~\cite{tfidf2014}. This statistic can be used as a feature extraction method by simply computing it for each feature (word frequency) of each sample (document). We describe the way the statistic is computed in this subsection.

The value of TF-IDF statistic is computed as the product of two other statistics -- term frequency and inverse document frequency~\cite{tfidf2014}. There are many ways to compute term frequency, we can either simply count number of words in a document (which we usually already have from previous steps of feature extraction) or choose a different more advanced possibility~\cite{manning2008introduction}. One such possibility is to use logarithmically scaled term frequency ($t$ is term, $d$ is document and $tf_{raw}$ is a function that returns number of terms $t$ in a document $d$):

$$tf_{log}(t,d) = 1 + log tf_{raw}(t,d)$$

Inverse document frequency is a statistic that determines relative rarity of a term across many documents~\cite{tfidf2014}. If, for example, a term ($t$) is always present in a set of documents ($D$), it conveys very little meaning and can be almost disregarded. This statistic weighs each term in this way allowing us to penalize words that are common and lessen their impact on our classifier. The value is computed simply by logarithmically scaling the total number of all documents in our dataset divided by number of documents that contain a term $t$:

$$idf(t, D) = log \frac{N}{|\{d \in D : t \in d\}|}$$

As mentioned above, the TF-IDF value for a term $t$, document $d$ from a set of documents $D$ can be then be evaluated by simply multiplying the term frequency $tf(t,d)$ and inverse document frequency $idf(t,D)$~\cite{manning2008introduction}:

$$tfidf(t,d,D) = tf(t,d) \times idf(t,D)$$

\subsection{Latent Semantic Indexing}

Latent semantic indexing (LSI) refers to a technique that uses singular value decomposition (SVD, \autoref{theorem:svd} taken from~\cite{manning2008introduction}) to reduce dimensionality of a term-document matrix (a matrix with columns representing document instances and rows representing terms). The resulting lower rank matrix yields to a new representation of the documents in the dataset~\cite{manning2008introduction}.

Consider a dataset $D$ that consists of 1000 bug report descriptions. These descriptions can sometimes be quite long therefore after you count all the words in them and create a feature vector (vector with number of occurrences for all unique terms/words in all documents), you usually discover there are about 6000-10000 unique words even if you remove all stop-words. This yields a term-document matrix $C$ with 1000 columns and 6000-10000 rows. Using dimensionality reduction such as LSI can reduce the number of rows as much as 20-fold without loosing much performance or even gaining some~\cite{Ahsan2009}. The number of columns is chosen by the user tuning parameter $k$.

\begin{theorem}
\label{theorem:svd}
Let $r$ be the rank (number of linearly independent rows) of the $C$ matrix with dimensions $M \times N$. There is a singular value decomposition of $C = U \Sigma V^T$, where:
\begin{enumerate}
    \item The eigenvalues $\lambda_1,...,\lambda_r$ are the same for both $CC^T$ and $C^TC$
    \item For $1 \leq i \leq r$, let the \textit{singular values} $\sigma_i = \sqrt{\lambda_i}$ with $\lambda_i \geq \lambda_{i+1}$. Then the $M \times N$ diagonal matrix $\Sigma$ is composed by setting $\Sigma_{ii} = \sigma_i$ for $1 \leq i \leq r$.
\end{enumerate}
\end{theorem}

The application of LSI to a term-document matrix $C$ is done by computing the SVD matrices in the form described in \autoref{theorem:svd} and then by finding matrix $C_k = U \Sigma_k V^T$, where $\Sigma_k$ is derived from $\Sigma$ by replacing all the $r-k$ smallest singular values by zeros. The resulting matrix $C_k$ has rank at most $k$ as $r-k$ values of the $\Sigma$ matrix were replaced by zeros~\cite{manning2008introduction}.

\subsection{Chi-Square Selection}

In statistics, the Chi-square test is used to examine independence of two events~\cite{Alenezi2013}. Similarly, we can use the test to rank all our terms by their independence with respect to classes and then set a threshold to select only $n$ features with the highest value of the Chi-square statistic. The Chi-square value is computed as follows~\cite{Alenezi2013}:

$$\chi^2 = \sum_{i=1}^m \sum_{j=1}^k \frac{(O_{ij} - E_{ij})^2}{E_{ij}}$$

Where $m$ is the number of intervals that are used for \textit{discretization} of features, $k$ is number of classes, $O_{ij}$ is the observed number of samples in the $i$th interval and $j$th class and $E_{ij}$ is the expected number of samples in the $i$th interval and $j$th class.