\chapter{Methodology}

This chapter describes the methodology of the evaluation as well as its application. It can be quite difficult to put together a good structure for a paper or a study that details some complicated research especially in a highly specialized field. With this in mind, we decided to opt for a framework proposed by Victor Basili of the University of Maryland, College Park and the Software Engineering Laboratory at the NASA Goddard Space Flight Center\todo{citation needed} called \textit{Goal Question Metric} (or just GQM).

\section{The Goal Question Metric Approach}

The focus of the GQM approach is to define a good measurement mechanism mainly for engineering disciplines like software engineering, computer science and others. Application of this approach helps support project planning, determine pros and cons of the current project and process, provide an intuition about the impact of modifying or refining a technique as well as assess current progress and last but not least to write the final work in a comprehensible and structured way.

The first prominent step in terms of GQM is to define a set of goals. Goals are entities that are to be assessed and therefore must be defined in a way that allows their assessment. Second step is to define a set of questions, these characterize the way the assessment of a goal is going to be carried out. The last step at the very bottom is to define metrics that are used to answer the questions in a quantitative way. the usual workflow is to define goals and then refine each into several separate questions. The questions are then further refined into metrics where more than one question can have the same metric in common.

\section{Application of GQM}

In this section, we apply the GQM methodology to our project. We proceed from top to bottom (i.e. from goals to metrics) as defined by Basili.

\subsection{Goals}

First, we define goals. In our case, there is only one general goal that looks like this:

\begin{framed}
  Goal 1:
  \begin{quote}
    Analyze \textbf{machine learning models} in order to \textbf{find the best possible approach} with respect to \textbf{assignee prediction} from the point of view of \textbf{project managers} in the context of \textbf{issue tracking systems}.
  \end{quote}
\end{framed}

\subsection{Questions}

Next, we need to refine questions for our single goal. We define seven questions, each one addressing a different concern of our research.

\begin{framed}
  Question 1:
  \begin{quote}
    Is the performance of the model better than performance of some baseline model?
  \end{quote}

  Question 2:
  \begin{quote}
    Does the model predict the correct assignee often enough?
  \end{quote}

  Question 3:
  \begin{quote}
    Does the model predict the correct assignee often enough even if the distribution of bug reports assigned to developers is imbalanced?
  \end{quote}

  Question 4:
  \begin{quote}
    Is the model general enough to work with all projects (a project means a dataset of bug reports)?
  \end{quote}

  Question 5:
  \begin{quote}
    Is there a difference between open source and proprietary data?
  \end{quote}

  Question 6:
  \begin{quote}
    Does the size of a window (a time period from which the data is collected) affect performance of the model?
  \end{quote}

  Question 7:
  \begin{quote}
    Does the number of predicted assignees affect performance?
  \end{quote}

\end{framed}

\subsection{Metrics}

The last step is to define metrics used to answer questions above. We use five metrics:

\begin{framed}
  Accuracy:
  \begin{quote}
    The amount of correctly predicted assignees over all tested predictions, formally:
    $$Accuracy = \frac{tp+tn}{tp+tn+fp+fn}$$
  \end{quote}

  Macro-Averaged Precision:
  \begin{quote}
    The amount of assignees correctly predicted as positive for given test sample over number of assignees either correctly or incorrectly predicted as positive, averaged over all classes\cite{Asch2013}, formally:
    $$Precision_{macro} = \frac{1}{q}\sum_{\lambda=1}^q\frac{tp_{\lambda}}{tp_{\lambda}+fp_{\lambda}}$$
  \end{quote}

  Macro-Averaged Recall:
  \begin{quote}
    The amount of assignees correctly predicted as positive for given test sample over number of assignees correctly predicted as positive or incorrectly predicted as negative, averaged over all classes\cite{Asch2013}, formally:
    $$Recall_{macro} = \frac{1}{q}\sum_{\lambda=1}^q\frac{tp_{\lambda}}{tp_{\lambda}+fn_{\lambda}}$$ 
  \end{quote}

  Chi-Squared Test:
  \begin{quote}
    Test to assert whether two samples are from the same distribution.
  \end{quote}

  T-Test:
  \begin{quote}
    Test to assert whether two samples have the same population mean.
  \end{quote}
\end{framed}