\chapter{Evaluation}

In this chapter, we present the results for various models on three datasets. First, we provide a description and origin of these datasets. Then we compare the performance of the models and the datasets. This includes comparison of open source data downloaded from the Internet with the proprietary data provided by the private company. In the third section, we compare the performance for different number of recommendations.\todo{What else? There should probably be more.}

\section{Datasets}

To evaluate performance of a model, a set of necessary data is necessary. We provide description of the three datasets that we chose for the evaluation of the models in this section.

\subsection{Firefox Data}

This dataset is downloaded from Mozilla repository\footnote{https://bugzilla.mozilla.org} in product Firefox. We downloaded all bugs that are in status \texttt{RESOLVED} with resolution \texttt{FIXED} and were created in year 2010 or later. We also removed bugs with field \texttt{assigned\_to} set to \texttt{nobody@mozilla.org} as those tickets were resolved and fixed but not assigned. This yields $9,141$ bugs in total. To get a better comparison with the other datasets, we only use $3,000$ datasets for training and cross-validation that were created between 2010-01-01 and 2012-07-10. This dataset contains $343$ labels (developers). Finally, we remove developers who did not fix at least $30$ bugs, yielding $1,810$ bugs with $20$ developers. The histogram with frequencies of the developers and cumulative distribution is on figure~\ref{fig:datasets.firefox.dist}.

\begin{figure}[htbp]
    \centering
        \includegraphics[width=\textwidth]{./images/distribution/firefox.png}
    \caption{Histogram and distribution of the Firefox data.}
    \label{fig:datasets.firefox.dist}
\end{figure}

\subsection{Netbeans Data}

Netbeans data were downloaded from Netbeans bug repository\footnote{https://netbeans.org/bugzilla/}. We considered latest $3,000$ bugs that are in status \texttt{RESOLVED} with resolution \texttt{FIXED}. We removed bugs with assignee \texttt{kenai\_tester\_git} as those were created automatically resulting in $2,924$ bug reports. These bugs were created between 2014-06-14 and 2015-06-14 and they contain $92$ developers. After removing developers who did not fix at least 30 bugs, the dataset contains $2,528$ reports with $30$ developers. Figure~\ref{fix:datasets.netbeans.dist} is a histogram and distribution of this datasets.

\begin{figure}[htbp]
    \centering
        \includegraphics[width=\textwidth]{./images/distribution/netbeans.png}
    \caption{Histogram and distribution of the Netbeans data.}
    \label{fig:datasets.netbeans.dist}
\end{figure}

\subsection{Proprietary Data}

The Proprietary dataset was provided by a company that wants to remain anonymous. The provided dataset contains $2,243$ bug reports created between 2009-03-07 and 2013-08-23. Only bug reports that were resolved with assigned developer were considered. There are $141$ developers in this dataset. Only $1,479$ bugs assigned to $28$ developers were retained after removal of developers with less than $30$ fixed bugs. The histogram and probability distribution is pictured on figure~\ref{fig:datasets.proprietary.dist}.

\begin{figure}[htbp]
    \centering
        \includegraphics[width=\textwidth]{./images/distribution/proprietary.png}
    \caption{Histogram and distribution of the Proprietary data.}
    \label{fig:datasets.proprietary.dist}
\end{figure}

\section{Stop-Words Removal}

Stop-words removal is a technique that can increases performance of the model. In this section, we determine whether the increase is significant enough to use it for subsequent comparisons of models.

Figure~\ref{fig:models.firefox.stopwords} shows the increase in performance after all stop-words were removed from the feature vector of the Firefox data. You can see that the performance of the classifier slightly increased on all models. Accuracy increased by 3\%, 0\% and 1\% on SVM, Naive Bayes and CART model respectively. Precision value of the SVM model decreased by 1\% but increased by 6\% with Naive Bayes and by 1\% with CART. Finally, Recall values of SVM and Naive Bayes increased by 4\% and 2\% respectively while it slightly decreased on the CART model by about 1\%.

\begin{figure}[htbp]
    \centering
        \includegraphics[width=\textwidth]{./images/comparison_of_models/firefox_0.png}
    \caption{Stop-Words removal on Firefox data.}
    \label{fig:models.firefox.stopwords}
\end{figure}

The results of the other datasets were similar in nature. We therefore conclude that the performance boost of stop-words removal is significant enough to warrant better results, which matches the conclusion of Čubranić and Murphy~\cite{Murphy} (see chapter~\ref{chapter:related-work}). Thus, in our evaluation, we always use stop-words removal in combination with other feature extraction/selection methods.

\section{Comparison of Models}

In the following text, comparison of the models is presented. We show the accuracy, precision and recall of these models on three datasets.\todo{What models? Explain or provide reference.}\todo{Explanation of these measurement metrics should be somewhere and perhaps a reference would be in order.} This comparison allows us to conclude what model is best for this application.

\subsection{Firefox Data}

On Figure~\ref{fig:results.models.firefox}, you can see the performance of the chosen models on Firefox data. The SVM model with TF-IDF weighing achieves the best performance with accuracy of 57\%. It's precision and recall also outperforms all the other approaches with values of 51\% and 45\%. The same model with LSI takes second place. The only model other than SVM that approaches SVM with it's performance is CART, especially with TF-IDF weighing.

\begin{figure}[htbp]
    \centering
        \includegraphics[width=\textwidth]{./images/comparison_of_models/firefox.png}
    \caption{Comparison of models on Firefox data.}
    \label{fig:results.models.firefox}
\end{figure}

\subsection{Netbeans Data}

The performance of the models on Netbeans data is shown on Figure~\ref{fig:results.models.netbeans}. It is clear that the SVM model with TF-IDF weighing performs best even on the Netbeans data, although in this case the LSI feature extraction technique does not seem to perform as well as in the previous case. The accuracy, precision and recall of the approach are 53\%, 53\% and 49\% respectively. Sole SVM model and SVM+$\chi^2$ model perform similarly as far as precision is concerned, while the accuracy and recall values are lagging behind by a considerable margin. None of the other models offer better performance than even sole SVM model on this data, which suggests that SVM is a very good choice in this domain.

\begin{figure}[htbp]
    \centering
        \includegraphics[width=\textwidth]{./images/comparison_of_models/netbeans.png}
    \caption{Comparison of models on Netbeans data.}
    \label{fig:results.models.netbeans}
\end{figure}

\subsection{Proprietary Data}

In this case, the performance of all models decreased a lot in comparison with the other datasets. However, even in this case, the SVM model with TF-IDF offers the best performance of 37\% accuracy, 32\% precision and 31\% recall. The same model with LSI also shows quite a good performance and quite surprisingly, the Naive Bayes model with LSI performs quite well as far as precision and recall is concerned. Figure~\ref{fig:results.models.proprietary} presents the results in graphical manner.

\begin{figure}[htbp]
    \centering
        \includegraphics[width=\textwidth]{./images/comparison_of_models/proprietary.png}
    \caption{Comparison of models on the Proprietary data.}
    \label{fig:results.models.proprietary}
\end{figure}

\subsection{Conclusion}

The comparison shows that SVM with TF-IDF weighing performs best on all datasets. Not only that, it also generalizes very well, because there was no need to readjust the parameters of the model to get the best or nearly the best performance for all datasets. The disadvantage of the model is that it is the most computationally complex one, because SVM is the slowest of the three models, there are a lot of classes and there are a lot of features. This can be at least partially dealt with by using $\chi^2$ feature extraction in conjunction with TF-IDF while sacrificing some of the performance.

\section{Comparison of Datasets}

The main purpose of this section is to compare proprietary dataset with open-source dataset. The open-source dataset used for this is from the Firefox project. We selected equal number of reports from both datasets to construct an accurate comparison.\todo{Maybe it would be useful to compare with the Netbeans data too.}\todo{And also it could be quite interesting to use the CART model too.}

\subsection{Naive Bayes Model}

The first model for comparison is Naive Bayes. The only used feature extraction method for this model is stop-words removal.

First plot (Figure \ref{fig:results.datasets.nb_accuracy}) represents accuracy of the Naive Bayes model. You can already see that the open source data perform better than the proprietary data. Accuracy of the classifier is 35\% vs 24\% when $x$ (see above) is set to 30. In the following text, we will always compare the results of this setting of parameter $x$ as we believe that is the best production value for the given size of the dataset.

\begin{figure}[htbp]
    \centering
        \includegraphics[width=\textwidth]{./images/prop_vs_os/nb_accuracy.png}
    \caption{Comparison of accuracy of Naive Bayes model.}
    \label{fig:results.datasets.nb_accuracy}
\end{figure}

Second plot (Figure \ref{fig:results.datasets.nb_pr}) is a representation of precision and recall of the same model. Precision of the classifier is 46\% and 22\% for the open source and proprietary data, respectively. Recall value is 29\% for the open source data and 19\% for the proprietary data.

\begin{figure}[htbp]
    \centering
        \includegraphics[width=\textwidth]{./images/prop_vs_os/nb_precision_and_recall.png}
    \caption{Comparison of precision and recall of Naive Bayes model.}
    \label{fig:results.datasets.nb_pr}
\end{figure}

\subsection{Support Vector Machine Model}

The second model used for comparison of the proprietary dataset with the open-source dataset is Support Vector Machine model with TF-IDF weighing and stop-words removal as this model shows the most promising results.

Figure \ref{fig:results.datasets.svm_accuracy} visualizes accuracy of the model. Even with this model, the open source data perform much better with accuracy value of 54\% vs 33\% of the proprietary data.

\begin{figure}[htbp]
    \centering
        \includegraphics[width=\textwidth]{./images/prop_vs_os/svm_accuracy.png}
    \caption{Comparison of accuracy of SVM model.}
    \label{fig:results.datasets.svm_accuracy}
\end{figure}

Precision and recall of the SVM model is pictured on figure \ref{fig:results.datasets.svm_pr}. It again clearly shows that the precision value of the open source data is higher (again about 54 \%) than the precision value of the proprietary data (32\%). The recall value is 49\% for the open-source data and 26\% for the proprietary data.

\begin{figure}[htbp]
    \centering
        \includegraphics[width=\textwidth]{./images/prop_vs_os/svm_precision_and_recall.png}
    \caption{Comparison of precision of SVM model.}
    \label{fig:results.datasets.svm_pr}
\end{figure}

\subsection{Conclusion}

The results clearly show that the open-source data provide much better performance. The number of classes is very similar and the number of reports used for training too, so it is clear that the quality of the open-source reports is higher.\todo{Or maybe there is something else? Try to figure out if that is really the case, if it is, it might be better to provide a better explanation.}

\section{Comparison of Performance for Variable Number of Recommendations}

We look at the performance of the models for different number of recommendations in this section. The plots will show the performance for number of recommendations from 1 to 10.\todo{More than one model might be in order.}

\subsection{Support Vector Machine Model}

For this comparison, we, again, take the SVM model with TF-IDF weighing and stop-words removed. Figure \ref{fig:results.topx.svm_accuracy} shows how the accuracy increases when the number of recommendations increases, which is expected as the more there are recommendation, the higher the chance of a hit is.\todo{Precision and recall would be useful too maybe.}

\begin{figure}[htbp]
    \centering
        \includegraphics[width=250px]{./images/top_x_comparison/svm_accuracy.png}
    \caption{Comparison of accuracy of top-x recommendations for SVM model.}
    \label{fig:results.topx.svm_accuracy}
\end{figure}

It is apparent from the plot that the highest performance boost happens when the number of recommendations changes from one to two both for the proprietary data and the Firefox data. 

\subsection{Conclusion}

From the results in this section, we concluded that the best number of recommendations to suggest to users is 5. The performance does not increase very much with more recommendations and it would probably only lead to confusion if the number of recommendations was too high.