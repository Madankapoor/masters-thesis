\chapter{Discussions}

In this chapter, we discuss the results and compare them with related works (see \autoref{chapter:related-work}). We also address some possible threats to validity and conclude the chapter with potential future work.

\section{Results}

In the beginning of \autoref{chapter:evaluation}, we analyzed the three datasets that support our subsequent evaluation of the models. The distribution of the datasets seen in \autoref{section:datasets} seems to be very similar and thus it is not surprising that we fail to reject both following hypotheses. In terms of our GQM \hyperlink{question:4}{question 4}, this suggests that our model should work with all datasets without the need to tune the model's parameters. Subsequent evaluation of performance of the datasets supports this conclusion further. It is, however, possible, that some highly specific datasets would not work very well or not at all and it would therefore be necessary to adjust some of the parameters of our classifier.

The distribution of the datasets in \autoref{section:datasets} and the consecutive testing of hypotheses also addresses \hyperlink{question:5}{question 5}. From this single point of view, it would seem there is almost no difference between open-source and proprietary data. In term of performance, however, there is a significant difference as can be seen in \autoref{section:comparison-of-datasets}. One possible explanation could be the size of the window that was used to select the training and testing samples of bug reports. If the window is too big, the developers that have a high number of resolved tickets but are no longer active could be considered by the model for future prediction and disadvantage the other developers that are currently active. This explanation does not seem to hold to our analysis as can be seen in \autoref{section:window-size}. As it was tested on open-source data, it is possible that the proprietary data are more sensitive to the window size. Second possible explanation could be bad quality of the proprietary data. It is, for example, possible, that the open source community pays more attention to ticket triage. Be it as it may, we can safely conclude that there is a difference between our samples of open-source and proprietary data albeit only in terms of performance.

As mentioned above, we evaluate \hyperlink{question:6}{question 6} in \autoref{section:window-size}. We employed three approaches to answer this question and while there was some variations in performance with respect to the size of the window, there was not found a trend that would warrant a conclusion in one way or another with high enough confidence. Even though the results do seem to suggest that the size of the window does not significantly change the performance of the classifier, it would be necessary to do a more thorough analysis with more than three approaches on more datasets to adopt this outcome with more confidence.

In \autoref{section:baseline}, we saw that our model outperforms the baseline model, which tells us that automatic ticket triage based on supervised text classification does work at least to some extent (\hyperlink{question:1}{question 1}). In the subsequent sections, we determined what model is the best in our case and what its performance is answering \hyperlink{question:2}{question 2}. We can also answered \hyperlink{question:3}{question 3} by evaluating precision and recall of the classifier. It is not very surprising that the Support Vector Machine model performs best as it was already concluded in many other works\todo{citation needed}. On the other hand, our selection of possible models is quite limited so while unlikely, it is possible that there are models that can outperform SVM on some or most of our datasets.

The last part (\autoref{section:compare-number-of-recommendations}) of our evaluation addresses the question of recommending more than one assignee for one bug report (\hyperlink{question:7}{question 7}). It is possible to assign a developer to a bug report automatically without any input from the reporter (or project manager) apart from the summary and description simply by having the classifier choose the most probable candidate. This way, the user who manages tickets has to rely on the computer generated model to successfully choose the correct developer. If that is not achieved, the bug report has to be reassigned to someone else. Another way is to utilize a semi-automated approach which presents the user with a list of suggestions rather than automatically filling in the best match. The question is how many developers to show in such a list without overwhelming the user with too many choices. The best approach might be to consider the change in performance, if the performance does not change much when the number of recommendations is changed from five to ten, it might not be sensible to recommend ten developers. One the other hand, it makes sense to change the number of suggestions to five if the performance increases significantly after recommending five possible assignees rather than two.

\section{Comparison with Related Works}

TODO

\section{Threats to Validity}

When doing research, it is quite often the case that the used approach has some questionable aspects. In this section, we discuss these aspects in depth starting with the metrics used for evaluation and continuing with the way the data is preprocessed, and finally the testing.

The three metrics used for evaluation has certain problems that should be kept in mind. All together, they are quite a good way to measure the performance of a machine learning model. Separately, however, they have to be treated with certain skepticism. Accuracy is the most obvious metrics when measuring how many times the correct answer is picked. There are many cases where this metric could be very misleading. One such case is when the labels of a dataset are heavily biased towards single class. Let's say that the dataset contains samples and 90\% of them are assigned the same label. In such a case the classifier could always predict the same label (in this case the label that is assigned to 90\% data samples) and easily achieve 90\% accuracy. If the label with these properties is the positive class, even precision would measure the same result. And recall would result in 100\%. It is therefore necessary to establish a baseline that for example always predict the most frequent class and consider accuracy in terms of the accuracy achieved by baseline. In our case, we achieved accuracy 18\% on Firefox data with our baseline while the accuracy of our best model on the same data is 57\%. This is a relatively good result even considering the performance of the baseline. On the other hand, it is still possible the model always chooses between only several classes instead of considering them all. 

In our evaluation, we also use precision and recall (both macro-averaged). These metrics must always be considered together as it is possible to achieve 100\% precision or recall if the datasets contains samples with only one class. Using macro-averaged variant of precision and recall makes this kind of misleading result somewhat less likely, it is not, however, eliminated altogether. Considering the baseline results for precision and recall, any bias towards a single class or several classes seems improbable but it cannot be ruled out. It should be kept in mind that these metrics are very theoretical and the reality can be a lot different.

Before the data we use for our evaluation is used for training, it is randomly shuffled. This increases the performance by as much as 8\%. We decided to shuffle the data beforehand to get more accurate results~\todo{add a reason why data is shuffled or add a section somewhere and make a reference}. This might be quite misleading, however. In reality, the classifier is used to predict a label for a new sample that was not created in the past. The new sample is usually as close to the time window of the training test as possible therefore the bias should not be too big. It is likely the most accurate result is somewhere between the results of the classifier trained on non-shuffled and then shuffled data.

The accuracy of our results can also by threatened by the way it is tested. We test our models only once, it is unfortunately the case that each test produces slightly different results sometimes by as much as 4\%. There are two reasons for this. First reason is the shuffling of the data as mentioned above. The way the models work makes them sensitive to the order of the data. Another reason is the way the models work themselves. The implementation of the SVM model, for example, randomly selects the best optimization for each iteration and it is of course not guaranteed each SVM session finds the most optimal results. This problem could be partially solved by training and testing the models more than once and using the expected value with confidence intervals instead of a raw value. Due to the time complexity of the models and the time it takes to do a single test, this was not realized in our evaluation.