\chapter{Discussions}

There were several goals of our thesis. First, we analyzed related work to get the possible candidate models for our own evaluation and analysis. Second objective was to evaluate the picked models to determine which one offers the best performance. Another goal was to analyze three datasets, one of which was provided by a private company, and to compare an open-source dataset with a proprietary dataset in terms of performance.

In this chapter, we discuss the results and address these goals in terms of GQM (chapter~\ref{chapter:methodology}). We also address some possible threats to validity, both internal and external, and we conclude the chapter comparing our results with related work (\autoref{chapter:related-work}).

\section{Results}

First of all, we analyzed the three datasets (Firefox, Netbeans datasets and one proprietary dataset) that support our subsequent evaluation of the models (\autoref{chapter:evaluation}). The distribution of the datasets (\autoref{section:datasets}) seems to be very similar and thus it is not surprising that we fail to reject both following hypotheses (T-Test and Chi-Square test). This suggests that our model should work with all datasets without the need to tune the model's parameters (\hyperlink{question:4}{Q4}). Subsequent evaluation of performance of the datasets using SVM and TF-IDF supports this conclusion further. Unfortunately, this conclusion does not apply for all feature extraction methods, our results show it is necessary to choose different parameters for both LSI and $\chi^2$ techniques. It is also possible that some highly specific datasets would not work so well even with SVM and TF-IDF making it necessary to adjust some of the parameters of the classifier.

The distribution of the datasets and the consecutive testing of hypotheses also contributes to the answer of \hyperlink{question:5}{Q5} (is there a difference between open-source and proprietary data?). From this point of view, it would seem there is almost no difference. In term of performance, however, there is a significant difference when we omit filtering of developers who fix few bugs (\autoref{section:comparison-of-datasets}). The most likely explanation, as provided in the conclusion of the section, is that the proprietary dataset contains less developers who fixed only a few bugs (because the proprietary bug repository is not available to the public).

The question of different window size and its effect on performance (\hyperlink{question:6}{Q6}) is addressed in \autoref{section:window-size}. We employed three approaches to answer this question and while there was some variations in performance with respect to the size of the window, we could not find a trend that would warrant a conclusion in one way or another with enough confidence. Even though the results do seem to suggest that the size of the window does not significantly change the performance of the classifier, it would be necessary to do a more thorough analysis with possibly different approaches on more datasets to adopt this outcome with higher confidence.

We saw that our model (SVM with TF-IDF) outperforms the baseline model (\autoref{section:baseline}), which tells us that automatic ticket triage based on supervised text classification does work at least to some extent (\hyperlink{question:1}{Q1}). In the subsequent sections, we determined what model is the best in our case and what its performance is answering \hyperlink{question:2}{Q2}. We also found out that our models work well on unbalanced data (\hyperlink{question:3}{Q3}) by evaluating precision and recall of the classifier. It is not very surprising that our model performs best as it was already concluded in many other works~\cite{Anvik2006}\cite{Ahsan2009}\cite{Somasundaram2012}. On the other hand, our selection of possible models is quite limited so while unlikely, it is possible that there are models that can outperform SVM on some or most of our datasets.

The last part of our evaluation (\autoref{section:compare-number-of-recommendations}) addresses the question of recommending more than one assignee for one bug report and the effects on performance (\hyperlink{question:7}{Q7}). It is possible to assign a developer to a bug report automatically without any input from the reporter (or project manager) apart from the summary and description simply by having the classifier choose the most probable candidate. This way, the user who manages tickets has to rely on the computer generated model to successfully choose the correct developer. If that is not achieved, the bug report has to be reassigned to someone else. Another way is to utilize a semi-automated approach which presents the user with a list of suggestions rather than automatically filling in the best match. The question is how many developers to show in such a list without overwhelming the user with too many choices. The best approach might be to consider the change in performance, if the performance does not change much when the number of recommendations is changed from five to ten, it might not be sensible to recommend ten developers. One the other hand, it makes sense to change the number of suggestions to five if the performance increases significantly after recommending five possible assignees rather than two.

\section{Threats to Validity}

Doing research is difficult and it can often lead to the use of approaches that have some questionable aspects. In this section, we discuss these aspects starting with external threats. There are two external threats, both concerning the metrics that are used. After that, we discuss the internal threats, one is a results of the way we pre-process the data and the other stems from the way the models are tested.

\subsection{External Threats}

The three metrics used for evaluation have certain problems that should be kept in mind. All together, they are quite a good way to measure the performance of a machine learning model. Separately, however, they have to be treated with certain skepticism. Accuracy is the most obvious metric when measuring how many times the correct answer is picked. There are many cases where this metric could be very misleading. One such case is when the labels of a dataset are heavily biased towards a single class. Let's say that the dataset contains samples and 90\% of them are assigned the same label. In such a case, the classifier could always predict the same label (in this case, the label that is assigned to 90\% data samples) and easily achieve 90\% accuracy. If the label with these properties is the positive class, even precision would measure the same result. And recall would result in 100\%. It is therefore necessary to establish a baseline that for example always predicts the most frequent class and consider accuracy in terms of the accuracy achieved by the chosen baseline. In our case, we achieved accuracy 18\% on Firefox data with our baseline while the accuracy of our best model on the same dataset is 57\%. This is a relatively good result considering the performance of the baseline. On the other hand, it is still possible the model always chooses between only several classes instead of considering them all. 

In our evaluation, we also use precision and recall (both macro-averaged). These metrics must always be considered together as it is possible to achieve 100\% precision or recall if a dataset contains samples with only one class. Using macro-averaged variant of precision and recall makes this sort of misleading result less likely. Considering the baseline results for precision and recall, any bias towards a single class or several classes seems improbable but it cannot be ruled out.

\subsection{Internal Threats}

Before our data is used for training, it is randomly shuffled. This increases the performance by as much as 8\%. We decided to shuffle the data beforehand to get more accurate results. This might be quite misleading, however. In reality, the classifier is used to predict a label for a new sample that was not created in the past. The new sample is usually as close to the time window of the training test as possible therefore the bias should not be too big. It is likely the most accurate result is somewhere between the results of the classifier trained on non-shuffled and then shuffled data.

The accuracy of our results can also be threatened by the way it is tested. We test our models only once, it is unfortunately the case that each test produces slightly different results sometimes by as much as 4\%. Apart from aforementioned shuffling, another reason is the way the models operate. The implementation of the SVM model, for example, randomly selects the best optimization for each iteration and it is of course not guaranteed each SVM session finds the most optimal results. This problem could be partially solved by training and testing the models more than once and using the expected value with confidence intervals instead of a raw value. Due to the time complexity of the models and the time it takes to do a single test, this extensive way of testing was not realized in our evaluation.

\section{Comparison with Related Work}

In this section, we attempt to compare our evaluation with related work. However, there are a couple of challenges that should be kept in mind. Some of the papers that we mention in related work (chapter \ref{chapter:related-work}) do not clearly define their evaluation process and even if they do, the differences in their process make it hard to draw a satisfactory conclusion. Another problem is that the sources of the datasets do not always match, and even if they do, it is unlikely they are from the same time window. It is our belief, based on the window analysis we did in chapter \ref{chapter:evaluation}, that the effect of different time windows on measured performance should be quite small. Lastly, some of the papers do not use the same metrics for evaluation. For these reasons, we compare our results with the results of only the papers we find sufficiently similar as far as the process, datasets and metrics are concerned.

First, we compare our results with Anvik et al.~\cite{Anvik2006}. In their evaluation, they were able to achieve precision 64\% and recall only 2\%. We were able to achieve precision 51\% and recall 45\%. There are a couple of differences, however. We used a dataset with 3,000 bug reports, they used 9,752. They also used a different approach of filtering inactive developers. The difference in recall can be probably attributed to the way it is calculated by Anvik.

Another study with which we compare our results is Alenezi et al.~\cite{Alenezi2013}, who used a Naive Bayes on several datasets as well as on Netbeans. The best results they achieved are with $\chi^2$ feature extraction---precision of 50\% and recall of 21\%. We achieved precision value 53\% and recall 49\% with SVM and TF-IDF. There are two major differences. Our dataset contains 3,000 bug reports while their contains 11,311. The other major difference is that while we remove developers that have not fixed at least 30 bugs, they remove developers that have not fixed at least 25 bugs in the last year.

Lastly, we compare our evaluation results with the work of Xia et al.~\cite{Xia2015} They employed an interesting learning algorithm multi-label k-nearest neighbor classifier (ML-kNN) and topic modeling using Latent Dirichlet Allocation (LDA). The datasets they used include Netbeans (26,000 samples) which makes it easier to compare with our results for Netbeans (3,000 samples). Their precision and recall values for top-5 recommendations are 32\% and 71\% respectively, while our precision and recall values for top-1 recommendation are 53\% and 49\%.

As mentioned in the beginning of this section, it is complicated to draw a conclusion when our processes, evaluations and datasets differ so much. However, the comparison can still be useful to better frame our contribution in the area of ticket triaging.