\chapter{Discussions}

In this chapter, we discuss the results and compare them with related work (see \autoref{chapter:related-work}). We also address some possible threats to validity and conclude the chapter with potential future work.

\section{Results}

In the beginning of \autoref{chapter:evaluation}, we analyzed the three datasets that support our subsequent evaluation of the models. The distribution of the datasets seen in \autoref{section:datasets} seems to be very similar and thus it is not surprising that we fail to reject both following hypotheses. In terms of our GQM \hyperlink{question:4}{question 4}, this suggests that our model should work with all datasets without the need to tune the model's parameters. Subsequent evaluation of performance of the datasets supports this conclusion further. It is, however, possible, that some highly specific datasets would not work very well or not at all and it would therefore be necessary to adjust some of the parameters of our classifier.

The distribution of the datasets in \autoref{section:datasets} and the consecutive testing of hypotheses also addresses \hyperlink{question:5}{question 5}. From this single point of view, it would seem there is almost no difference between open-source and proprietary data. In term of performance, however, there is a significant difference as can be seen in \autoref{section:comparison-of-datasets}. One possible explanation could be the size of the window that was used to select the training and testing samples of bug reports. If the window is too big, the developers that have a high number of resolved tickets but are no longer active could be considered by the model for future prediction and disadvantage the other developers that are currently active. This explanation does not seem to hold to our analysis as can be seen in \autoref{section:window-size}. As it was tested on open-source data, it is possible that the proprietary data are more sensitive to the window size. Second possible explanation could be bad quality of the proprietary data. It is, for example, possible, that the open source community pays more attention to ticket triage. Be it as it may, we can safely conclude that there is a difference between our samples of open-source and proprietary data albeit only in terms of performance.

As mentioned above, we evaluate \hyperlink{question:6}{question 6} in \autoref{section:window-size}. We employed three approaches to answer this question and while there was some variations in performance with respect to the size of the window, there was not found a trend that would warrant a conclusion in one way or another with high enough confidence. Even though the results do seem to suggest that the size of the window does not significantly change the performance of the classifier, it would be necessary to do a more thorough analysis with more than three approaches on more datasets to adopt this outcome with more confidence.

In \autoref{section:baseline}, we saw that our model outperforms the baseline model, which tells us that automatic ticket triage based on supervised text classification does work at least to some extent (\hyperlink{question:1}{question 1}). In the subsequent sections, we determined what model is the best in our case and what its performance is answering \hyperlink{question:2}{question 2}. We can also answered \hyperlink{question:3}{question 3} by evaluating precision and recall of the classifier. It is not very surprising that the Support Vector Machine model performs best as it was already concluded in many other works\todo{citation needed}. On the other hand, our selection of possible models is quite limited so while unlikely, it is possible that there are models that can outperform SVM on some or most of our datasets.

The last part (\autoref{section:compare-number-of-recommendations}) of our evaluation addresses the question of recommending more than one assignee for one bug report (\hyperlink{question:7}{question 7}). It is possible to assign a developer to a bug report automatically without any input from the reporter (or project manager) apart from the summary and description simply by having the classifier choose the most probable candidate. This way, the user who manages tickets has to rely on the computer generated model to successfully choose the correct developer. If that is not achieved, the bug report has to be reassigned to someone else. Another way is to utilize a semi-automated approach which presents the user with a list of suggestions rather than automatically filling in the best match. The question is how many developers to show in such a list without overwhelming the user with too many choices. The best approach might be to consider the change in performance, if the performance does not change much when the number of recommendations is changed from five to ten, it might not be sensible to recommend ten developers. One the other hand, it makes sense to change the number of suggestions to five if the performance increases significantly after recommending five possible assignees rather than two.

\section{Threats to Validity}

When doing research, it is quite often the case that the used approach has some questionable aspects. In this section, we discuss these aspects in depth starting with the metrics used for evaluation and continuing with the way the data is preprocessed, and finally the testing.

The three metrics used for evaluation have certain problems that should be kept in mind. All together, they are quite a good way to measure the performance of a machine learning model. Separately, however, they have to be treated with certain skepticism. Accuracy is the most obvious metric when measuring how many times the correct answer is picked. There are many cases where this metric could be very misleading. One such case is when the labels of a dataset are heavily biased towards a single class. Let's say that the dataset contains samples and 90\% of them are assigned the same label. In such a case, the classifier could always predict the same label (in this case, the label that is assigned to 90\% data samples) and easily achieve 90\% accuracy. If the label with these properties is the positive class, even precision would measure the same result. And recall would result in 100\%. It is therefore necessary to establish a baseline that for example always predicts the most frequent class and consider accuracy in terms of the accuracy achieved by the chosen baseline. In our case, we achieved accuracy 18\% on Firefox data with our baseline while the accuracy of our best model on the same dataset is 57\%. This is a relatively good result considering the performance of the baseline. On the other hand, it is still possible the model always chooses between only several classes instead of considering them all. 

In our evaluation, we also use precision and recall (both macro-averaged). These metrics must always be considered together as it is possible to achieve 100\% precision or recall if the datasets contains samples with only one class. Using macro-averaged variant of precision and recall makes this sort of misleading result somewhat less likely, it is not, however, eliminated altogether. Considering the baseline results for precision and recall, any bias towards a single class or several classes seems improbable but it cannot be ruled out. It should be kept in mind that these metrics are very theoretical and the reality can be a lot different.

Before our data is used for training, it is randomly shuffled. This increases the performance by as much as 8\%. We decided to shuffle the data beforehand to get more accurate results~\todo{add a reason why data is shuffled or add a section somewhere and make a reference}. This might be quite misleading, however. In reality, the classifier is used to predict a label for a new sample that was not created in the past. The new sample is usually as close to the time window of the training test as possible therefore the bias should not be too big. It is likely the most accurate result is somewhere between the results of the classifier trained on non-shuffled and then shuffled data.

The accuracy of our results can also by threatened by the way it is tested. We test our models only once, it is unfortunately the case that each test produces slightly different results sometimes by as much as 4\%. There are two reasons for this. First reason is the shuffling of the data as mentioned above. The way the models work makes them sensitive to the order of the data. Another reason is the way the models operate themselves. The implementation of the SVM model, for example, randomly selects the best optimization for each iteration and it is of course not guaranteed each SVM session finds the most optimal results. This problem could be partially solved by training and testing the models more than once and using the expected value with confidence intervals instead of a raw value. Due to the time complexity of the models and the time it takes to do a single test, this extensive way of testing was not realized in our evaluation.

\section{Comparison with Related Work}

In this section, we attempt to compare our evaluation with related work. However, there are a couple of challenges that should be kept in mind. Some of the papers that we mention in chapter \ref{chapter:related-work} do not clearly define their evaluation process and even if they do, the differences in their process make it hard to draw a satisfactory conclusion. Another problem is that the sources of the datasets used by us and them do not always match, and even if they do, it is unlikely they are from the same time window. It is our belief, based on the window analysis we did in chapter \ref{chapter:evaluation}, that the effect of different time windows on measured performance should be quite small. Lastly, some of the papers we mention in chapter \ref{chapter:related-work} do not use the same metrics for evaluation. For these reasons, we compare our results with the results of only the papers we find sufficiently similar as far as the process, datasets and metrics are concerned.

First, we compare our results with Anvik et al.~\cite{Anvik2006}. In their evaluation, they were able to achieve precision 64\% and recall only 2\%. We were able to achieve precision 51\% and recall 45\%. There are a couple of differences, however. We used a dataset with 3,000 bug reports, they used 9,752. They also used a different approach of filtering inactive developers. The difference in recall can be probably attributed to the way it is calculated by Anvik.

Another study with which we compare our results is Alenezi et al.~\cite{Alenezi2013}, who used a Naive Bayes on several datasets as well as on Netbeans. The best results they achieved are with $\chi^2$ feature extraction -- precision of 50\% and recall of 21\%. We achieved precision value 53\% and recall 49\% with SVM and TF-IDF. There are two major differences. Our dataset contains 3,000 bug reports while their contains 11,311. The other major difference is that while we remove developers that have not fixed at least 30 bugs, they remove developers that have not fixed at least 25 bugs in the last year.

Lastly, we compare our evaluation results with the work of Xia et al.~\cite{Xia2015} They employed an interesting learning algorithm multi-label k-nearest neighbor classifier (ML-kNN) and topic modeling using Latent Dirichlet Allocation (LDA). The datasets they used include Netbeans (26,000 samples) which makes it easier to compare with our results for Netbeans (3,000 samples). Their precision and recall values for top-5 recommendations are 32\% and 71\% respectively, while our precision and recall values for top-1 recommendation are 53\% and 49\%.

As mentioned in the beginning of this section, it is complicated to draw a conclusion when our processes, evaluations and datasets differ so much. We can, however, conclude that both our and their results are similar which implies our process is at least as good as theirs. 